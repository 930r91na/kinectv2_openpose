#include "OpenPoseInterface.h"
#include <spdlog/spdlog.h>
#include <filesystem>
#include <fstream>
#include <sstream>
#include <thread>
#include <mutex>
#include <queue>
#include <condition_variable>
#include <atomic>
#include <chrono>
#include <iomanip>
#include <algorithm>
#include <set>

namespace fs = std::filesystem;

// Implementation class
class OpenPoseInterfaceImpl {
public:
    OpenPoseInterface::Configuration config;
    explicit OpenPoseInterfaceImpl(OpenPoseInterface::Configuration config)
        : config(std::move(config)), initialized(false) {

        // Create required directories if they don't exist
        try {
            if (!fs::exists(config.tempDirectory)) {
                fs::create_directories(config.tempDirectory);
            }

            if (!fs::exists(config.outputJsonDirectory)) {
                fs::create_directories(config.outputJsonDirectory);
            }
        } catch (const fs::filesystem_error& e) {
            spdlog::warn("Error creating directories: {}", e.what());
        }

        // Initialize joint mapping between OpenPose and Kinect
        initJointMapping();
    }

    ~OpenPoseInterfaceImpl() {
        // Clean up temporary directories
        try {
            if (fs::exists(config.tempDirectory)) {
                fs::remove_all(config.tempDirectory);
            }

            if (fs::exists(config.outputJsonDirectory)) {
                fs::remove_all(config.outputJsonDirectory);
            }
        } catch (const fs::filesystem_error& e) {
            spdlog::warn("Error cleaning temporary directories: {}", e.what());
        }
    }

    // Initialize the interface
    bool initialize() {
        // Check if OpenPose executable exists
        if (!fs::exists(config.openPoseExePath)) {
            spdlog::error("OpenPose executable not found at: {}",
                         config.openPoseExePath.string());
            return false;
        }

        spdlog::info("OpenPoseInterface initialized with executable: {}",
                    config.openPoseExePath.string());
        initialized = true;
        return true;
    }

    bool isInitialized() const noexcept {
        return initialized;
    }

    // Process a single frame
    std::optional<std::vector<OpenPoseInterface::Person3D>> processFrame(
        const cv::Mat& colorImage,
        const cv::Mat& depthImage,
        ICoordinateMapper* coordinateMapper
    ) {
        if (!initialized) {
            spdlog::error("Cannot process frame: OpenPoseInterface not initialized");
            return std::nullopt;
        }

        if (colorImage.empty() || depthImage.empty() || !coordinateMapper) {
            spdlog::error("Invalid inputs to processFrame");
            return std::nullopt;
        }

        // Create a unique timestamp for this frame
        auto timestamp = std::chrono::system_clock::now().time_since_epoch().count();
        fs::path imagePath = config.tempDirectory / ("frame_" + std::to_string(timestamp) + ".png");

        // Ensure temp directory exists
        try {
            if (!fs::exists(config.tempDirectory)) {
                fs::create_directories(config.tempDirectory);
            }
        } catch (const fs::filesystem_error& e) {
            spdlog::error("Failed to create temp directory: {}", e.what());
            return std::nullopt;
        }

        // Save the color image to temp directory
        try {
            if (!cv::imwrite(imagePath.string(), colorImage)) {
                spdlog::error("Failed to save color image to temp directory");
                return std::nullopt;
            }
        } catch (const cv::Exception& e) {
            spdlog::error("OpenCV error saving image: {}", e.what());
            return std::nullopt;
        }

        // Run OpenPose on the image
        if (!runOpenPoseOnImage(config.tempDirectory, config.outputJsonDirectory)) {
            try {
                fs::remove(imagePath);
            } catch (const fs::filesystem_error&) {
                // Ignore cleanup errors
            }
            return std::nullopt;
        }

        // Get the latest JSON file generated by OpenPose
        fs::path jsonFile = getLastJsonFile(config.outputJsonDirectory);
        if (jsonFile.empty()) {
            spdlog::error("No JSON output found from OpenPose");
            try {
                fs::remove(imagePath);
            } catch (const fs::filesystem_error&) {
                // Ignore cleanup errors
            }
            return std::nullopt;
        }

        // Read and parse the JSON
        nlohmann::json openposeData = readOpenPoseJson(jsonFile);

        // Clean up temp files
        try {
            fs::remove(imagePath);
            fs::remove(jsonFile);
        } catch (const fs::filesystem_error&) {
            // Ignore cleanup errors
        }

        // Check if any people were detected
        if (openposeData.empty() || !openposeData.contains("people") ||
            openposeData["people"].empty()) {
            spdlog::debug("No people detected in the frame");
            return std::vector<OpenPoseInterface::Person3D>{};
        }

        // Process 3D lifting
        std::vector<OpenPoseInterface::Person3D> people;
        if (!process3DLifting(colorImage, depthImage, coordinateMapper, openposeData, people)) {
            spdlog::warn("Failed to process 3D lifting");
            return std::nullopt;
        }

        return people;
    }

    // Load frame timestamps from CSV file
    std::vector<std::pair<int, int64_t>> loadFrameTimestamps(const fs::path& recordingDir) {
        std::vector<std::pair<int, int64_t>> frameTimestamps;
        fs::path timestampPath = recordingDir / "frame_timestamps.csv";

        if (!fs::exists(timestampPath)) {
            spdlog::warn("Frame timestamps file not found: {}", timestampPath.string());
            return frameTimestamps;
        }

        try {
            std::ifstream file(timestampPath);
            if (!file.is_open()) {
                spdlog::error("Failed to open timestamps file: {}", timestampPath.string());
                return frameTimestamps;
            }

            // Skip header line
            std::string line;
            std::getline(file, line);

            // Read each line: frame_index,timestamp_ns,elapsed_ms,delta_ms
            while (std::getline(file, line)) {
                std::stringstream ss(line);
                std::string item;

                // Parse frame index
                std::getline(ss, item, ',');
                int frameIndex = std::stoi(item);

                // Parse timestamp
                std::getline(ss, item, ',');
                int64_t timestamp = std::stoll(item);

                frameTimestamps.emplace_back(frameIndex, timestamp);
            }

            spdlog::info("Loaded {} frame timestamps", frameTimestamps.size());
        } catch (const std::exception& e) {
            spdlog::error("Error loading frame timestamps: {}", e.what());
        }

        return frameTimestamps;
    }

    // Identify unique frames based on timestamps (remove duplicates)
    std::vector<int> identifyUniqueFrames(const std::vector<std::pair<int, int64_t>>& timestamps) {
        std::vector<int> uniqueFrames;

        if (timestamps.empty()) {
            return uniqueFrames;
        }

        // Add the first frame
        uniqueFrames.push_back(timestamps.front().first);

        // Use a minimum delta time threshold (in ms) to identify new frames
        // For 10 FPS, frames should be about 100ms apart
        const int64_t MIN_DELTA_NS = 50 * 1000000; // 50ms in nanoseconds

        int64_t lastTimestamp = timestamps.front().second;

        for (size_t i = 1; i < timestamps.size(); i++) {
            int64_t currentTimestamp = timestamps[i].second;
            int64_t delta = currentTimestamp - lastTimestamp;

            // If the time difference is significant, this is a new frame
            if (delta >= MIN_DELTA_NS) {
                uniqueFrames.push_back(timestamps[i].first);
                lastTimestamp = currentTimestamp;
            }
        }

        spdlog::info("Identified {} unique frames from {} total frames",
                     uniqueFrames.size(), timestamps.size());
        return uniqueFrames;
    }

    OpenPoseInterface::ProcessingResult processRecording(
        const fs::path& recordingDir,
        ICoordinateMapper* coordinateMapper,
        const fs::path& outputDir,
        int numThreads,
        OpenPoseInterface::ProgressCallback progressCallback,
        bool cleanupTempFiles
    ) {

    OpenPoseInterface::ProcessingResult result;
    result.outputDirectory = outputDir;

    auto startTime = std::chrono::high_resolution_clock::now();

    // Basic validation
    if (!initialized || !fs::exists(recordingDir) || !coordinateMapper) {
        spdlog::error("Cannot process recording: Invalid parameters or not initialized");
        return result;
    }

    // Create output directories
    try {
        fs::create_directories(outputDir);
        fs::create_directories(outputDir / "json");
        fs::create_directories(outputDir / "viz");
    } catch (const fs::filesystem_error& e) {
        spdlog::error("Failed to create output directories: {}", e.what());
        return result;
    }

    // STEP 1: Determine which source to use - prioritize processing_temp
    fs::path processingTempDir = recordingDir / "processing_temp";
    fs::path colorDir = recordingDir / "color";
    fs::path videoPath = recordingDir / "color.mp4";

    // Simply check which source exists and use the best one available
    fs::path frameSourceDir;
    bool useVideo = false;

    if (fs::exists(processingTempDir)) {
        frameSourceDir = processingTempDir;
        spdlog::info("Using pre-extracted frames from processing_temp directory");
    } else if (fs::exists(colorDir)) {
        frameSourceDir = colorDir;
        spdlog::info("Using frames from color directory");
    } else if (fs::exists(videoPath)) {
        useVideo = true;
        spdlog::info("Using video file for frame extraction");
    } else {
        spdlog::error("No valid frame source found in {}", recordingDir.string());
        return result;
    }

    // STEP 2: Find frames to process
    std::vector<int> framesToProcess;

    // First check for process_frames.txt as it contains the list of original frames
    fs::path processFramesPath = recordingDir / "process_frames.txt";
    if (fs::exists(processFramesPath)) {
        try {
            std::ifstream frameListFile(processFramesPath);
            if (frameListFile.is_open()) {
                std::string line;
                while (std::getline(frameListFile, line)) {
                    try {
                        framesToProcess.push_back(std::stoi(line));
                    } catch (...) {
                        // Skip invalid lines
                    }
                }
                spdlog::info("Found {} frames in process_frames.txt", framesToProcess.size());
            }
        } catch (const std::exception& e) {
            spdlog::warn("Error reading process_frames.txt: {}", e.what());
        }
    }

    // If no frames list found, scan the directory for frame files
    if (framesToProcess.empty() && !useVideo) {
        for (const auto& entry : fs::directory_iterator(frameSourceDir)) {
            if (entry.path().extension() == ".png") {
                std::string filename = entry.path().filename().string();
                size_t underscorePos = filename.find('_');
                size_t dotPos = filename.find('.');

                if (underscorePos != std::string::npos && dotPos != std::string::npos) {
                    std::string indexStr = filename.substr(underscorePos + 1, dotPos - underscorePos - 1);
                    try {
                        framesToProcess.push_back(std::stoi(indexStr));
                    } catch (...) {
                        // Skip files with invalid indices
                    }
                }
            }
        }
        spdlog::info("Found {} frames in directory scan", framesToProcess.size());
    } else if (framesToProcess.empty() && useVideo) {
        // For video, we need to sample frames if no list is available
        cv::VideoCapture tempCapture(videoPath.string());
        if (tempCapture.isOpened()) {
            int totalFrames = static_cast<int>(tempCapture.get(cv::CAP_PROP_FRAME_COUNT));
            // Sample every 3rd frame as a reasonable default
            for (int i = 0; i < totalFrames; i += 3) {
                framesToProcess.push_back(i);
            }
            tempCapture.release();
            spdlog::info("Sampling {} frames from video", framesToProcess.size());
        }
    }

    // Sort frames and check if we found any
    std::sort(framesToProcess.begin(), framesToProcess.end());
    if (framesToProcess.empty()) {
        spdlog::error("No frames identified for processing");
        return result;
    }

    // STEP 3: Process frames in batches
    int totalFrames = framesToProcess.size();
    int processedFrames = 0;
    int peopleDetected = 0;

    // Create a temporary directory for batch processing
    fs::path tempFramesDir = outputDir / "temp_frames";
    fs::create_directories(tempFramesDir);

    // Open video if needed
    cv::VideoCapture videoCapture;
    if (useVideo) {
        videoCapture.open(videoPath.string());
        if (!videoCapture.isOpened()) {
            spdlog::error("Failed to open video file: {}", videoPath.string());
            return result;
        }
    }

    // Process in batches for better performance
    const int batchSize = std::min(config.batchSize, 50);

    for (int i = 0; i < totalFrames; i += batchSize) {
        int batchEnd = std::min(i + batchSize, totalFrames);
        std::vector<std::pair<int, cv::Mat>> batchFrames;

        // Load frames for this batch
        for (int j = i; j < batchEnd; j++) {
            int frameIdx = framesToProcess[j];
            cv::Mat colorImage;

            // Load from the appropriate source
            if (useVideo) {
                videoCapture.set(cv::CAP_PROP_POS_FRAMES, frameIdx);
                if (!videoCapture.read(colorImage)) {
                    spdlog::warn("Failed to extract frame {} from video", frameIdx);
                    continue;
                }
            } else {
                std::string framePath = (frameSourceDir / ("frame_" + std::to_string(frameIdx) + ".png")).string();
                if (!fs::exists(framePath)) {
                    spdlog::warn("Frame {} not found in {}", frameIdx, frameSourceDir.string());
                    continue;
                }

                colorImage = cv::imread(framePath);
                if (colorImage.empty()) {
                    spdlog::warn("Failed to load frame {}", frameIdx);
                    continue;
                }
            }

            // Add to batch
            batchFrames.emplace_back(frameIdx, colorImage);

            // Save to temp directory for batch processing
            std::string tempPath = (tempFramesDir / ("frame_" + std::to_string(frameIdx) + ".png")).string();
            cv::imwrite(tempPath, colorImage);
        }

        // Process the batch with OpenPose
        if (!batchFrames.empty()) {
            if (runOpenPoseOnImage(tempFramesDir, config.outputJsonDirectory)) {
                // Process each frame's results
                for (const auto& [frameIdx, colorImage] : batchFrames) {
                    // Find corresponding JSON output
                    fs::path jsonPath;
                    for (const auto& entry : fs::directory_iterator(config.outputJsonDirectory)) {
                        if (entry.path().extension() == ".json" &&
                            entry.path().string().find(std::to_string(frameIdx)) != std::string::npos) {
                            jsonPath = entry.path();
                            break;
                        }
                    }

                    if (fs::exists(jsonPath)) {
                        // Load depth data
                        std::string depthPath = (recordingDir / "depth_raw" / ("frame_" + std::to_string(frameIdx) + ".bin")).string();
                        if (!fs::exists(depthPath)) {
                            continue;
                        }

                        cv::Mat depthImage = loadRawDepthData(depthPath);
                        if (depthImage.empty()) {
                            continue;
                        }

                        // Process 3D lifting
                        nlohmann::json openposeData = readOpenPoseJson(jsonPath);
                        std::vector<OpenPoseInterface::Person3D> people;

                        if (process3DLifting(colorImage, depthImage, coordinateMapper, openposeData, people)) {
                            // Save results
                            fs::path outputJsonPath = outputDir / "json" / ("frame_" + std::to_string(frameIdx) + ".json");
                            OpenPoseInterface::save3DSkeletonToJson(people, outputJsonPath);

                            // Create visualization
                            cv::Mat visualized = OpenPoseInterface::visualize3DSkeleton(colorImage, people);
                            fs::path outputVizPath = outputDir / "viz" / ("frame_" + std::to_string(frameIdx) + ".png");
                            cv::imwrite(outputVizPath.string(), visualized);

                            peopleDetected += people.size();
                            processedFrames++;
                        }
                    }
                }
            }
        }

        // Clean temporary files for this batch
        try {
            for (const auto& entry : fs::directory_iterator(tempFramesDir)) {
                fs::remove(entry.path());
            }

            for (const auto& entry : fs::directory_iterator(config.outputJsonDirectory)) {
                fs::remove(entry.path());
            }
        } catch (...) {
            // Ignore cleanup errors
        }

        // Update progress
        if (progressCallback) {
            progressCallback(i + (batchEnd - i), totalFrames);
        }

        spdlog::info("Processing progress: {:.1f}% ({}/{})",
                   (100.0f * batchEnd) / totalFrames, batchEnd, totalFrames);
    }

    // STEP 4: Clean up
    if (useVideo) {
        videoCapture.release();
    }

    try {
        fs::remove_all(tempFramesDir);
        fs::remove_all(config.tempDirectory);
        fs::remove_all(config.outputJsonDirectory);

        // Delete processing_temp directory if requested
        if (cleanupTempFiles && fs::exists(processingTempDir)) {
            spdlog::info("Cleaning up processing_temp directory");
            fs::remove_all(processingTempDir);
        }
    } catch (const fs::filesystem_error& e) {
        spdlog::warn("Error during cleanup: {}", e.what());
    }

    // Calculate results
    auto endTime = std::chrono::high_resolution_clock::now();
    std::chrono::duration<double> duration = endTime - startTime;

    result.framesProcessed = processedFrames;
    result.peopleDetected = peopleDetected;
    result.processingTimeSeconds = duration.count();

    spdlog::info("Processing complete: {} frames processed in {:.2f} seconds",
               processedFrames, duration.count());

    return result;
}

private:
    // Initialize joint mapping
    void initJointMapping() {
        // Map OpenPose BODY_25 keypoints to Kinect joint types
        openposeToKinectJointMap = {
            {0, JointType_Head},            // Nose -> Head
            {1, JointType_Neck},            // Neck -> Neck
            {2, JointType_ShoulderRight},   // RShoulder -> ShoulderRight
            {3, JointType_ElbowRight},      // RElbow -> ElbowRight
            {4, JointType_WristRight},      // RWrist -> WristRight
            {5, JointType_ShoulderLeft},    // LShoulder -> ShoulderLeft
            {6, JointType_ElbowLeft},       // LElbow -> ElbowLeft
            {7, JointType_WristLeft},       // LWrist -> WristLeft
            {8, JointType_SpineBase},       // MidHip -> SpineBase
            {9, JointType_HipRight},        // RHip -> HipRight
            {10, JointType_KneeRight},      // RKnee -> KneeRight
            {11, JointType_AnkleRight},     // RAnkle -> AnkleRight
            {12, JointType_HipLeft},        // LHip -> HipLeft
            {13, JointType_KneeLeft},       // LKnee -> KneeLeft
            {14, JointType_AnkleLeft},      // LAnkle -> AnkleLeft
            {15, JointType_Head},           // REye -> Head (approximate)
            {16, JointType_Head},           // LEye -> Head (approximate)
            {17, JointType_Head},           // REar -> Head (approximate)
            {18, JointType_Head},           // LEar -> Head (approximate)
            // Body_25 has additional points (19-24) for feet that don't map well to Kinect
        };
    }

    // Run OpenPose with the video
    bool runOpenPoseOnVideo(const fs::path& videoPath, const fs::path& outputDir) const {
        // Extract the OpenPose root directory from the executable path
        std::string openPoseDir = config.openPoseExePath.string();
        const size_t binPos = openPoseDir.find("\\bin\\");
        if (binPos == std::string::npos) {
            spdlog::error("Cannot determine OpenPose root directory from path: {}",
                        config.openPoseExePath.string());
            return false;
        }
        openPoseDir = openPoseDir.substr(0, binPos);

        // Get absolute paths for input and output
        fs::path absVideoPath = fs::absolute(videoPath);
        fs::path absOutputDir = fs::absolute(outputDir);

        // Make sure output directory exists
        try {
            fs::create_directories(absOutputDir);
        } catch (const fs::filesystem_error& e) {
            spdlog::error("Failed to create output directory: {}", e.what());
            return false;
        }

        // Build command that first changes to OpenPose directory then runs the executable
        std::stringstream cmd;
        cmd << "cd /d \"" << openPoseDir << "\" && "
            << "bin\\OpenPoseDemo.exe"
            << " --video \"" << absVideoPath.string() << "\""  // Use video directly
            << " --write_json \"" << absOutputDir.string() << "\""
            << " --display 0"
            << " --render_pose 0"
            << " --number_people_max 1"
            << " --disable_blending true";

        // Add performance settings
        if (config.performanceMode) {
            cmd << " --net_resolution -1x256";
            cmd << " --model_pose BODY_25";
            cmd << " --maximize_positives false";
        } else if (config.useMaximumAccuracy) {
            cmd << " --net_resolution 1312x736"
                << " --scale_number 4"
                << " --scale_gap 0.25";
        } else {
            cmd << " --net_resolution -1x" << config.netResolution;
        }

        spdlog::info("Running OpenPose command: {}", cmd.str());

        // Execute command
        int result = std::system(cmd.str().c_str());

        if (result != 0) {
            spdlog::error("OpenPose execution failed with code: {}", result);
            return false;
        }

        return true;
    }

    // Run OpenPose on an image or directory
    bool runOpenPoseOnImage(const fs::path& imagePath, const fs::path& outputDir) const {
        // Extract the OpenPose root directory from the executable path
        std::string openPoseDir = config.openPoseExePath.string();
        const size_t binPos = openPoseDir.find("\\bin\\");
        if (binPos == std::string::npos) {
            spdlog::error("Cannot determine OpenPose root directory from path: {}",
                        config.openPoseExePath.string());
            return false;
        }
        openPoseDir = openPoseDir.substr(0, binPos);

        // Get absolute paths for input and output directories
        fs::path absImagePath = fs::absolute(imagePath);
        fs::path absOutputDir = fs::absolute(outputDir);

        // Build command that first changes to OpenPose directory then runs the executable
        std::stringstream cmd;
        cmd << "cd /d \"" << openPoseDir << "\" && "
            << "bin\\OpenPoseDemo.exe"  // Use relative path since we're already in the OpenPose directory
            << " --image_dir \"" << absImagePath.string() << "\""
            << " --write_json \"" << absOutputDir.string() << "\""
            << " --display 0"
            << " --render_pose 0"
            << " --number_people_max 1" // Limit to one person for faster processing
            << " --disable_blending true"; // Disable blending for better performance

        // Performance settings
        if (config.performanceMode) {
            // OPTIMIZATION: These settings sacrifice some accuracy for much faster processing
            cmd << " --net_resolution -1x256"; // Lower resolution
            cmd << " --model_pose BODY_25";  // Stick with BODY_25 model for joint compatibility
            cmd << " --maximize_positives false";    // Skip low confidence detections
        } else if (config.useMaximumAccuracy) {
            // Use high accuracy settings
            cmd << " --net_resolution 1312x736"
                << " --scale_number 4"
                << " --scale_gap 0.25";
        } else {
            // Default balanced settings
            cmd << " --net_resolution -1x" << config.netResolution;
        }

        spdlog::debug("Running OpenPose command: {}", cmd.str());

        // Execute command
        int result = std::system(cmd.str().c_str());

        if (result != 0) {
            spdlog::error("OpenPose execution failed with code: {}", result);
            return false;
        }

        return true;
    }

    // Read OpenPose JSON output
    static nlohmann::json readOpenPoseJson(const fs::path& jsonPath) {
        try {
            std::ifstream file(jsonPath);
            if (!file.is_open()) {
                spdlog::error("Failed to open JSON file: {}", jsonPath.string());
                return nlohmann::json{};
            }

            nlohmann::json j;
            file >> j;
            return j;
        } catch (const std::exception& e) {
            spdlog::error("Error parsing JSON: {}", e.what());
            return nlohmann::json{};
        }
    }

    // Get the latest JSON file in a directory
    static fs::path getLastJsonFile(const fs::path& directory) {
        fs::path latestFile;

        try {
            std::time_t latestTime = 0;
            for (const auto& entry : fs::directory_iterator(directory)) {
                if (entry.path().extension() == ".json") {
                    auto fileTime = fs::last_write_time(entry.path());
                    auto timePoint = std::chrono::time_point_cast<std::chrono::system_clock::duration>(
                        fileTime - fs::file_time_type::clock::now() + std::chrono::system_clock::now());
                    auto time = std::chrono::system_clock::to_time_t(timePoint);

                    if (time > latestTime) {
                        latestTime = time;
                        latestFile = entry.path();
                    }
                }
            }
        } catch (const fs::filesystem_error& e) {
            spdlog::error("Error accessing directory: {}", e.what());
        }

        return latestFile;
    }

    // Load raw depth data from file
    static cv::Mat loadRawDepthData(const fs::path& depthPath) {
        try {
            std::ifstream depthFile(depthPath, std::ios::binary);
            if (!depthFile.is_open()) {
                spdlog::error("Failed to open depth file: {}", depthPath.string());
                return {};
            }

            // Read dimensions
            int rows = 0, cols = 0;
            depthFile.read(reinterpret_cast<char*>(&rows), sizeof(int));
            depthFile.read(reinterpret_cast<char*>(&cols), sizeof(int));

            // Validate dimensions
            if (rows <= 0 || cols <= 0 || rows > 10000 || cols > 10000) {
                spdlog::error("Invalid depth dimensions: {}x{}", rows, cols);
                return {};
            }

            // Allocate matrix
            cv::Mat depthImg(rows, cols, CV_16UC1);

            // Read data
            depthFile.read(reinterpret_cast<char*>(depthImg.data),
                        depthImg.elemSize() * depthImg.total());

            // Check if read was successful
            if (depthFile.fail()) {
                spdlog::error("Failed to read depth data");
                return cv::Mat();
            }

            depthFile.close();
            return depthImg;
        } catch (const std::exception& e) {
            spdlog::error("Error loading depth data: {}", e.what());
            return cv::Mat();
        }
    }

    // Process 3D lifting from 2D OpenPose data and depth info
    bool process3DLifting(
        const cv::Mat& colorImage,
        const cv::Mat& depthImage,
        ICoordinateMapper* coordinateMapper,
        const nlohmann::json& openposeData,
        std::vector<OpenPoseInterface::Person3D>& detectedPeople
    ) const {
        if (colorImage.empty() || depthImage.empty() || !coordinateMapper || openposeData.empty()) {
            spdlog::error("Invalid inputs to process3DLifting");
            return false;
        }

        // Ensure depth image is 16-bit
        cv::Mat depthMat16U;
        if (depthImage.type() != CV_16UC1) {
            depthImage.convertTo(depthMat16U, CV_16UC1, 65535.0/255.0);
        } else {
            depthMat16U = depthImage;
        }

        // Pre-map the entire color frame to depth space for efficiency
        const int depthWidth = depthImage.cols;
        const int depthHeight = depthImage.rows;
        const int depthSize = depthWidth * depthHeight;

        // Create depth to camera mapping table
        std::vector<DepthSpacePoint> colorToDepthMap(colorImage.cols * colorImage.rows);

        // Get depth frame data
        std::vector<UINT16> depthData(depthSize);
        for (int y = 0; y < depthHeight; y++) {
            for (int x = 0; x < depthWidth; x++) {
                depthData[y * depthWidth + x] = depthMat16U.at<UINT16>(y, x);
            }
        }

        // Map color frame to depth space
        HRESULT hr = coordinateMapper->MapColorFrameToDepthSpace(
            depthSize,
            depthData.data(),
            colorToDepthMap.size(),
            colorToDepthMap.data());

        if (FAILED(hr)) {
            spdlog::error("Failed to map color frame to depth space");
            return false;
        }

        // Process each detected person
        detectedPeople.clear();

        if (!openposeData.contains("people") || openposeData["people"].empty()) {
            spdlog::debug("No people detected in OpenPose data");
            return false;
        }

        for (const auto& person : openposeData["people"]) {
            OpenPoseInterface::Person3D person3D;

            // Get 2D keypoints from OpenPose
            if (!person.contains("pose_keypoints_2d") || person["pose_keypoints_2d"].empty()) {
                continue;
            }

            const auto& keypoints2d = person["pose_keypoints_2d"];

            // Prepare for depth filtering
            std::vector<float> validDepths;

            // Process each keypoint
            for (size_t i = 0; i < keypoints2d.size() / 3; i++) {
                float x = keypoints2d[i*3];
                float y = keypoints2d[i*3 + 1];
                float confidence = keypoints2d[i*3 + 2];

                // Skip low confidence points
                if (confidence < config.keypointConfidenceThreshold / 100.0f) {
                    OpenPoseInterface::Keypoint3D kp;
                    kp.x = x;  // Store original 2D coordinates for visualization
                    kp.y = y;
                    kp.z = 0;
                    kp.confidence = 0;
                    person3D.keypoints.push_back(kp);
                    continue;
                }

                // Initialize keypoint with original 2D coordinates
                OpenPoseInterface::Keypoint3D kp;
                kp.x = x;
                kp.y = y;
                kp.z = 0;
                kp.confidence = confidence;

                // Check if point is within color image bounds
                if (x >= 0 && x < colorImage.cols && y >= 0 && y < colorImage.rows) {
                    // Get corresponding depth point from pre-calculated mapping
                    int colorIndex = static_cast<int>(y) * colorImage.cols + static_cast<int>(x);

                    if (colorIndex >= 0 && colorIndex < static_cast<int>(colorToDepthMap.size())) {
                        DepthSpacePoint depthPoint = colorToDepthMap[colorIndex];

                        // Check if mapping is valid
                        if (depthPoint.X >= 0 && depthPoint.Y >= 0) {
                            int depthX = static_cast<int>(depthPoint.X);
                            int depthY = static_cast<int>(depthPoint.Y);

                            // Check if within depth image bounds
                            if (depthX >= 0 && depthX < depthMat16U.cols &&
                                depthY >= 0 && depthY < depthMat16U.rows) {

                                // Get depth value at this point
                                UINT16 depth = depthMat16U.at<UINT16>(depthY, depthX);

                                // Only process valid depth values
                                if (depth > 0) {
                                    // Convert to camera space
                                    CameraSpacePoint cameraPoint;
                                    hr = coordinateMapper->MapDepthPointToCameraSpace(
                                        depthPoint, depth, &cameraPoint);

                                    if (SUCCEEDED(hr)) {
                                        // Store 3D coordinates in world space
                                        kp.world_x = cameraPoint.X;
                                        kp.world_y = cameraPoint.Y;
                                        kp.world_z = cameraPoint.Z;

                                        // Keep 2D coordinates in image space for visualization
                                        // but store depth as z
                                        kp.z = cameraPoint.Z;

                                        // Store valid depths for outlier detection
                                        validDepths.push_back(cameraPoint.Z);
                                    }
                                }
                            }
                        }
                    }
                }

                // Add keypoint
                person3D.keypoints.push_back(kp);
            }

            // Post-processing for stability - filter obvious outliers
            if (!person3D.keypoints.empty() && !validDepths.empty()) {
                // Calculate median depth
                std::sort(validDepths.begin(), validDepths.end());
                const float medianDepth = validDepths[validDepths.size() / 2];

                // Reject outliers (points more than 50% from median)
                for (auto& kp : person3D.keypoints) {
                    if (kp.confidence > 0 && kp.z > 0) {
                        if (std::abs(kp.z - medianDepth) > medianDepth * 0.5f) {
                            // Replace with more reliable estimate
                            kp.z = medianDepth;
                            if (kp.world_z > 0) {
                                kp.world_z = medianDepth;
                            }
                        }
                    }
                }
            }

            // Add person if they have keypoints
            if (!person3D.keypoints.empty()) {
                detectedPeople.push_back(std::move(person3D));
            }
        }

        return !detectedPeople.empty();
    }

    // Member variables
    bool initialized;
    std::unordered_map<int, JointType> openposeToKinectJointMap;
};

//
// OpenPoseInterface implementation
//

OpenPoseInterface::OpenPoseInterface(Configuration config)
    : pImpl(std::make_unique<OpenPoseInterfaceImpl>(std::move(config))) {
}

OpenPoseInterface::~OpenPoseInterface() = default;

OpenPoseInterface::OpenPoseInterface(OpenPoseInterface&&) noexcept = default;
OpenPoseInterface& OpenPoseInterface::operator=(OpenPoseInterface&&) noexcept = default;

bool OpenPoseInterface::initialize() {
    return pImpl->initialize();
}

bool OpenPoseInterface::isInitialized() const noexcept {
    return pImpl->isInitialized();
}

void OpenPoseInterface::setConfiguration(const Configuration& config) {
    pImpl = std::make_unique<OpenPoseInterfaceImpl>(config);
}

const OpenPoseInterface::Configuration& OpenPoseInterface::getConfiguration() const {
    return pImpl->config;
}

std::optional<std::vector<OpenPoseInterface::Person3D>> OpenPoseInterface::processFrame(
    const cv::Mat& colorImage,
    const cv::Mat& depthImage,
    ICoordinateMapper* coordinateMapper
) {
    return pImpl->processFrame(colorImage, depthImage, coordinateMapper);
}


std::future<OpenPoseInterface::ProcessingResult> OpenPoseInterface::processRecordingAsync(
    const std::filesystem::path& recordingDir,
    ICoordinateMapper* coordinateMapper,
    const std::filesystem::path& outputDir,
    int numThreads,
    ProgressCallback progressCallback,
    bool cleanupTempFiles
) {
    return std::async(std::launch::async,
        [this, recordingDir, coordinateMapper, outputDir, numThreads, progressCallback, cleanupTempFiles]() {
            return pImpl->processRecording(recordingDir, coordinateMapper, outputDir,
                                          numThreads, progressCallback, cleanupTempFiles);
        });
}

OpenPoseInterface::ProcessingResult OpenPoseInterface::processRecording(
    const std::filesystem::path& recordingDir,
    ICoordinateMapper* coordinateMapper,
    const std::filesystem::path& outputDir,
    int numThreads,
    ProgressCallback progressCallback,
    bool cleanupTempFiles
) {
    return pImpl->processRecording(recordingDir, coordinateMapper, outputDir,
                                  numThreads, progressCallback, cleanupTempFiles);
}



bool OpenPoseInterface::save3DSkeletonToJson(
    const std::vector<Person3D>& people,
    const std::filesystem::path& outputPath
) {
    try {
        nlohmann::json output;
        output["people"] = nlohmann::json::array();

        for (const auto& person : people) {
            nlohmann::json personJson;
            nlohmann::json keypointsJson = nlohmann::json::array();

            for (const auto& kp : person.keypoints) {
                // Use world coordinates for accurate 3D representation
                keypointsJson.push_back(kp.world_x > 0 ? kp.world_x : kp.x);
                keypointsJson.push_back(kp.world_y > 0 ? kp.world_y : kp.y);
                keypointsJson.push_back(kp.world_z > 0 ? kp.world_z : kp.z);
                keypointsJson.push_back(kp.confidence);
            }

            personJson["pose_keypoints_3d"] = keypointsJson;
            output["people"].push_back(personJson);
        }

        // Create directory if needed
        fs::create_directories(outputPath.parent_path());

        // Write to file
        std::ofstream file(outputPath);
        if (!file.is_open()) {
            spdlog::error("Failed to open file for writing: {}", outputPath.string());
            return false;
        }

        file << output.dump(4);
        file.close();
        return true;
    } catch (const std::exception& e) {
        spdlog::error("Failed to save 3D skeleton: {}", e.what());
        return false;
    }
}

cv::Mat OpenPoseInterface::visualize3DSkeleton(
    const cv::Mat& image,
    const std::vector<Person3D>& people
) {
    // Make a copy of the input image
    cv::Mat result = image.clone();

    // Define the skeleton connections
    static const std::vector<std::pair<int, int>> connections = {
        {0, 1}, {1, 2}, {1, 5}, {1, 8}, {2, 3}, {3, 4}, {5, 6},
        {6, 7}, {8, 9}, {8, 12}, {9, 10}, {10, 11}, {12, 13}, {13, 14}
    };

    // Add a title with frame information
    cv::putText(result, "3D Skeleton Reconstruction",
               cv::Point(10, 30), cv::FONT_HERSHEY_SIMPLEX, 0.8,
               cv::Scalar(0, 0, 255), 2);

    for (const auto& person : people) {
        const auto& keypoints = person.keypoints;

        // Draw connections first (so points are on top)
        for (const auto& connection : connections) {
            int start = connection.first;
            int end = connection.second;

            if (start >= keypoints.size() || end >= keypoints.size()) {
                continue;
            }

            const auto& kp1 = keypoints[start];
            const auto& kp2 = keypoints[end];

            if (kp1.confidence < 0.1f || kp2.confidence < 0.1f) {
                continue;
            }

            // Make sure the 2D coordinates are within image bounds
            cv::Point p1(static_cast<int>(kp1.x), static_cast<int>(kp1.y));
            cv::Point p2(static_cast<int>(kp2.x), static_cast<int>(kp2.y));

            if (p1.x < 0 || p1.x >= result.cols || p1.y < 0 || p1.y >= result.rows ||
                p2.x < 0 || p2.x >= result.cols || p2.y < 0 || p2.y >= result.rows) {
                continue;
            }

            // Average z value for the line color
            float avgZ = (kp1.z + kp2.z) / 2.0f;
            if (avgZ <= 0) avgZ = 2.0f; // Default depth if not available

            // Normalize depth for visualization
            float normalizedZ = std::min(avgZ / 5.0f, 1.0f);

            cv::Scalar color(255 * (1.0f - normalizedZ), 0, 255 * normalizedZ);
            cv::line(result, p1, p2, color, 2);
        }

        // Draw keypoints
        for (size_t i = 0; i < keypoints.size(); i++) {
            const auto& kp = keypoints[i];

            // Skip invalid points
            if (kp.confidence < 0.1f) continue;

            // Make sure the point is within image bounds
            if (kp.x < 0 || kp.x >= result.cols || kp.y < 0 || kp.y >= result.rows) {
                continue;
            }

            // Draw the keypoint - the z value affects the color (red to blue)
            // Normalize z between 0 and 1 for visualization
            float zDepth = kp.z > 0 ? kp.z : 2.0f; // Default depth if not available
            float normalizedZ = std::min(zDepth / 5.0f, 1.0f);

            cv::Scalar color(255 * (1.0f - normalizedZ), 0, 255 * normalizedZ);
            cv::circle(result, cv::Point(static_cast<int>(kp.x), static_cast<int>(kp.y)),
                      5, color, -1);

            // Draw keypoint ID and depth value
            std::stringstream ss;
            ss << i << ":" << std::fixed << std::setprecision(2) << kp.z << "m";
            cv::putText(result, ss.str(),
                      cv::Point(static_cast<int>(kp.x) + 10, static_cast<int>(kp.y)),
                      cv::FONT_HERSHEY_SIMPLEX, 0.4, cv::Scalar(0, 255, 0), 1);
        }
    }

    // Add a depth scale bar
    int scaleWidth = 30;
    int scaleHeight = 200;
    int scaleX = result.cols - scaleWidth - 20;
    int scaleY = 50;

    // Draw depth scale gradient
    for (int y = 0; y < scaleHeight; y++) {
        float normalizedZ = 1.0f - (float)y / scaleHeight;
        cv::Scalar color(255 * (1.0f - normalizedZ), 0, 255 * normalizedZ);
        cv::rectangle(result,
                     cv::Point(scaleX, scaleY + y),
                     cv::Point(scaleX + scaleWidth, scaleY + y + 1),
                     color, -1);
    }

    // Add labels to scale
    cv::putText(result, "0m", cv::Point(scaleX + scaleWidth + 5, scaleY + scaleHeight),
               cv::FONT_HERSHEY_SIMPLEX, 0.5, cv::Scalar(255, 255, 255), 1);
    cv::putText(result, "5m", cv::Point(scaleX + scaleWidth + 5, scaleY),
               cv::FONT_HERSHEY_SIMPLEX, 0.5, cv::Scalar(255, 255, 255), 1);
    cv::putText(result, "Depth", cv::Point(scaleX, scaleY - 20),
               cv::FONT_HERSHEY_SIMPLEX, 0.5, cv::Scalar(255, 255, 255), 1);

    return result;
}